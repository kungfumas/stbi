{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://towardsdatascience.com/create-a-simple-search-engine-using-python-412587619ff5\nhttps://medium.com/@deangela.neves/how-to-build-a-search-engine-from-scratch-in-python-part-1-96eb240f9ecb\n\nhttps://towardsdatascience.com/how-to-build-a-search-engine-9f8ffa405eac","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bs4","metadata":{"execution":{"iopub.status.busy":"2021-09-08T11:38:56.545095Z","iopub.execute_input":"2021-09-08T11:38:56.545586Z","iopub.status.idle":"2021-09-08T11:41:27.297018Z","shell.execute_reply.started":"2021-09-08T11:38:56.545543Z","shell.execute_reply":"2021-09-08T11:41:27.295733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup# Make a request to the website\nr = requests.get('https://bola.kompas.com/')# Create an object to parse the HTML format\nsoup = BeautifulSoup(r.content, 'html.parser')# Retrieve all popular news links (Fig. 1)\nlink = []\nfor i in soup.find('div', {'class':'most__wrap'}).find_all('a'):\n    i['href'] = i['href'] + '?page=all'\n    link.append(i['href'])# For each link, we retrieve paragraphs from it, combine each paragraph as one string, and save it to documents (Fig. 2)\ndocuments = []\nfor i in link:\n    # Make a request to the link\n    r = requests.get(i)\n  \n    # Initialize BeautifulSoup object to parse the content \n    soup = BeautifulSoup(r.content, 'html.parser')\n  \n    # Retrieve all paragraphs and combine it as one\n    sen = []\n    for i in soup.find('div', {'class':'read__content'}).find_all('p'):\n        sen.append(i.text)\n  \n    # Add the combined paragraphs to documents\n    documents.append(' '.join(sen))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T11:36:57.968172Z","iopub.execute_input":"2021-09-08T11:36:57.968566Z","iopub.status.idle":"2021-09-08T11:37:18.034083Z","shell.execute_reply.started":"2021-09-08T11:36:57.968532Z","shell.execute_reply":"2021-09-08T11:37:18.030698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pip install sastrawi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# import Sastrawi package\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\n# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\n# stem\nsentence = 'Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan'\noutput   = stemmer.stem(sentence)\n\nprint(output)\n# ekonomi indonesia sedang dalam tumbuh yang bangga","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\nlower_case = kalimat.lower()\nprint(lower_case)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re # impor modul regular expression\nkalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\nhasil = re.sub(r\"\\d+\", \"\", kalimat)\nprint(hasil)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        \n        return document","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_text(kalimat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kalimat = \"rumah idaman adalah rumah yang bersih\"\npisah = kalimat.split()\nprint(pisah)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize \n \nkalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\"\n \ntokens = nltk.tokenize.word_tokenize(kalimat)\nprint(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nkalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\nkalimat = kalimat.translate(str.maketrans('','')).lower()\n \ntokens = nltk.tokenize.word_tokenize(kalimat)\nkemunculan = nltk.FreqDist(tokens)\nprint(kemunculan.most_common())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nkemunculan.plot(30,cumulative=False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport pandas as pd\n# set of documents\ntrain = ['The sky is blue.','The sun is bright.']\ntest = ['The sun in the sky is bright', 'We can see the shining sun, the bright sun.']\n# instantiate the vectorizer object\ncountvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\ntfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n# convert th documents into a matrix\ncount_wm = countvectorizer.fit_transform(train)\ntfidf_wm = tfidfvectorizer.fit_transform(train)\n#retrieve the terms found in the corpora\n# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n#count_tokens = tfidfvectorizer.get_feature_names() # no difference\ncount_tokens = countvectorizer.get_feature_names()\ntfidf_tokens = tfidfvectorizer.get_feature_names()\ndf_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\ndf_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\nprint(\"Count Vectorizer\\n\")\nprint(df_countvect)\nprint(\"\\nTD-IDF Vectorizer\\n\")\nprint(df_tfidfvect)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T14:45:09.483581Z","iopub.execute_input":"2021-08-21T14:45:09.483916Z","iopub.status.idle":"2021-08-21T14:45:10.447503Z","shell.execute_reply.started":"2021-08-21T14:45:09.483862Z","shell.execute_reply":"2021-08-21T14:45:10.446734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncolumns = ['kalimat', 'label']\nrows = []\n\nrows = [['Ini buku-buku saya', 'berita'], \n        ['Buku-buku saya merupakan novel', 'berita'],\n        ['apakah kamu sudah membaca buku-buku itu?', 'pertanyaan'],\n        ['siapakah pengarang buku tersebut?', 'pertanyaan'],\n        ['siapa saja yang ada di buku tersebut?', 'pertanyaan'],\n        ['Saya membeli buku tersebut secara online', 'berita'],\n        ['Saya suka membaca buku cerita', 'berita'],\n        ['Apakah buku kesukaanmu?', 'pertanyaan'],\n       ['Membeli buku secara online lebih mudah', 'berita'],\n        ['Buku cerita sangat menghibur', 'berita'],\n        ['apakah kamu suka buku cerita?', 'pertanyaan']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:17:55.805826Z","iopub.execute_input":"2021-11-15T13:17:55.806353Z","iopub.status.idle":"2021-11-15T13:17:55.841167Z","shell.execute_reply.started":"2021-11-15T13:17:55.806319Z","shell.execute_reply":"2021-11-15T13:17:55.840129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mengubah label kelas berita=0 dan yang lain menjadi =1\ntraining_data['labelkelas'] = training_data['label'].apply(lambda x: 0 if x=='berita' else 1)\ntraining_data\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:20:24.182427Z","iopub.execute_input":"2021-11-15T13:20:24.182986Z","iopub.status.idle":"2021-11-15T13:20:24.19925Z","shell.execute_reply.started":"2021-11-15T13:20:24.182951Z","shell.execute_reply":"2021-11-15T13:20:24.198469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#memecah data test 30% dari keseluruhan data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(training_data['kalimat'], training_data['labelkelas'], test_size = 0.3,random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:21:44.741075Z","iopub.execute_input":"2021-11-15T13:21:44.741613Z","iopub.status.idle":"2021-11-15T13:21:45.698288Z","shell.execute_reply.started":"2021-11-15T13:21:44.741579Z","shell.execute_reply":"2021-11-15T13:21:45.697054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:22:19.963336Z","iopub.execute_input":"2021-11-15T13:22:19.963693Z","iopub.status.idle":"2021-11-15T13:22:19.971094Z","shell.execute_reply.started":"2021-11-15T13:22:19.963661Z","shell.execute_reply":"2021-11-15T13:22:19.969861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:22:54.090193Z","iopub.execute_input":"2021-11-15T13:22:54.090827Z","iopub.status.idle":"2021-11-15T13:22:54.097111Z","shell.execute_reply.started":"2021-11-15T13:22:54.090789Z","shell.execute_reply":"2021-11-15T13:22:54.096406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"#mengubah menjadi vector term\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:23:40.509513Z","iopub.execute_input":"2021-11-15T13:23:40.510042Z","iopub.status.idle":"2021-11-15T13:23:40.540988Z","shell.execute_reply.started":"2021-11-15T13:23:40.510002Z","shell.execute_reply":"2021-11-15T13:23:40.539858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_cv[1:1]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:24:44.154888Z","iopub.execute_input":"2021-11-15T13:24:44.15555Z","iopub.status.idle":"2021-11-15T13:24:44.162942Z","shell.execute_reply.started":"2021-11-15T13:24:44.155509Z","shell.execute_reply":"2021-11-15T13:24:44.161761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pembuatan model KNN\nfrom sklearn.neighbors import KNeighborsClassifier \nclassifier = KNeighborsClassifier(n_neighbors=3) \nclassifier.fit(X_train_cv, y_train)\n#prediksi menggunakan data test\ny_pred = classifier.predict(X_test_cv)\n#menghitung confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix \nprint(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:26:01.670932Z","iopub.execute_input":"2021-11-15T13:26:01.671359Z","iopub.status.idle":"2021-11-15T13:26:01.86014Z","shell.execute_reply.started":"2021-11-15T13:26:01.671324Z","shell.execute_reply":"2021-11-15T13:26:01.859166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:27:37.57658Z","iopub.execute_input":"2021-11-15T13:27:37.577024Z","iopub.status.idle":"2021-11-15T13:27:37.585912Z","shell.execute_reply.started":"2021-11-15T13:27:37.57698Z","shell.execute_reply":"2021-11-15T13:27:37.584444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:27:53.052776Z","iopub.execute_input":"2021-11-15T13:27:53.05318Z","iopub.status.idle":"2021-11-15T13:27:53.059974Z","shell.execute_reply.started":"2021-11-15T13:27:53.053148Z","shell.execute_reply":"2021-11-15T13:27:53.059093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, y_pred))\nprint('Precision score: ', precision_score(y_test, y_pred))\nprint('Recall score: ', recall_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:29:59.336062Z","iopub.execute_input":"2021-11-15T13:29:59.336727Z","iopub.status.idle":"2021-11-15T13:29:59.349998Z","shell.execute_reply.started":"2021-11-15T13:29:59.336663Z","shell.execute_reply":"2021-11-15T13:29:59.348678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['pertanyaan', 'berita'], yticklabels=['pertanyaan', 'berita'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:30:10.062334Z","iopub.execute_input":"2021-11-15T13:30:10.062864Z","iopub.status.idle":"2021-11-15T13:30:10.350097Z","shell.execute_reply.started":"2021-11-15T13:30:10.06283Z","shell.execute_reply":"2021-11-15T13:30:10.348937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(20):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train_cv,y_train)\n    Y_pred_dt = dt.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\n\ndt = DecisionTreeClassifier(random_state=best_x)\ndt.fit(X_train_cv,y_train)\nY_pred_dt = dt.predict(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:10:38.759059Z","iopub.execute_input":"2021-09-12T06:10:38.75939Z","iopub.status.idle":"2021-09-12T06:10:38.83166Z","shell.execute_reply.started":"2021-09-12T06:10:38.759362Z","shell.execute_reply":"2021-09-12T06:10:38.830003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix \nprint(confusion_matrix(y_test, Y_pred_dt)) \nprint(classification_report(y_test, Y_pred_dt))","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:12:20.100839Z","iopub.execute_input":"2021-09-12T06:12:20.101179Z","iopub.status.idle":"2021-09-12T06:12:20.117543Z","shell.execute_reply.started":"2021-09-12T06:12:20.101143Z","shell.execute_reply":"2021-09-12T06:12:20.116015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncm = confusion_matrix(y_test,Y_pred_dt)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['pertanyaan', 'berita'], yticklabels=['pertanyaan', 'berita'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:13:29.954023Z","iopub.execute_input":"2021-09-12T06:13:29.954367Z","iopub.status.idle":"2021-09-12T06:13:30.040425Z","shell.execute_reply.started":"2021-09-12T06:13:29.954338Z","shell.execute_reply":"2021-09-12T06:13:30.039071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncolumns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question'],\n       ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:48:44.368716Z","iopub.execute_input":"2021-08-21T12:48:44.369203Z","iopub.status.idle":"2021-08-21T12:48:44.400686Z","shell.execute_reply.started":"2021-08-21T12:48:44.36917Z","shell.execute_reply":"2021-08-21T12:48:44.399873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data['label'] = training_data['class'].apply(lambda x: 0 if x=='stmt' else 1)\ntraining_data","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:50:47.104228Z","iopub.execute_input":"2021-08-21T12:50:47.104693Z","iopub.status.idle":"2021-08-21T12:50:47.122259Z","shell.execute_reply.started":"2021-08-21T12:50:47.104655Z","shell.execute_reply":"2021-08-21T12:50:47.121272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(training_data['sent'], training_data['label'], test_size = 0.3,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:50:54.132642Z","iopub.execute_input":"2021-08-21T12:50:54.133098Z","iopub.status.idle":"2021-08-21T12:50:55.179331Z","shell.execute_reply.started":"2021-08-21T12:50:54.133059Z","shell.execute_reply":"2021-08-21T12:50:55.178386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:50:59.804137Z","iopub.execute_input":"2021-08-21T12:50:59.804839Z","iopub.status.idle":"2021-08-21T12:50:59.830623Z","shell.execute_reply.started":"2021-08-21T12:50:59.804779Z","shell.execute_reply":"2021-08-21T12:50:59.829204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nscaler = StandardScaler() \nscaler.fit(X_train_cv)\nX_train_cv = scaler.transform(X_train_cv) \nX_test_cv = scaler.transform(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:51:09.491105Z","iopub.execute_input":"2021-08-21T12:51:09.491537Z","iopub.status.idle":"2021-08-21T12:51:09.57355Z","shell.execute_reply.started":"2021-08-21T12:51:09.491503Z","shell.execute_reply":"2021-08-21T12:51:09.572162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_freq = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())\ntop_words_df = pd.DataFrame(word_freq.sum()).sort_values(0, ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:04:22.741158Z","iopub.execute_input":"2021-08-21T13:04:22.741568Z","iopub.status.idle":"2021-08-21T13:04:22.750689Z","shell.execute_reply.started":"2021-08-21T13:04:22.741533Z","shell.execute_reply":"2021-08-21T13:04:22.749236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, y_train)\npredictions = naive_bayes.predict(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:39:45.072595Z","iopub.execute_input":"2021-09-12T06:39:45.073028Z","iopub.status.idle":"2021-09-12T06:39:45.086216Z","shell.execute_reply.started":"2021-09-12T06:39:45.072996Z","shell.execute_reply":"2021-09-12T06:39:45.084982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions))\nprint('Precision score: ', precision_score(y_test, predictions))\nprint('Recall score: ', recall_score(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:39:59.987317Z","iopub.execute_input":"2021-09-12T06:39:59.987707Z","iopub.status.idle":"2021-09-12T06:40:00.006826Z","shell.execute_reply.started":"2021-09-12T06:39:59.987677Z","shell.execute_reply":"2021-09-12T06:40:00.006031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['pertanyaan', 'berita'], yticklabels=['pertanyaan', 'berita'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T06:40:49.285175Z","iopub.execute_input":"2021-09-12T06:40:49.285767Z","iopub.status.idle":"2021-09-12T06:40:49.371515Z","shell.execute_reply.started":"2021-09-12T06:40:49.285733Z","shell.execute_reply":"2021-09-12T06:40:49.370708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier \nclassifier = KNeighborsClassifier(n_neighbors=5) \nclassifier.fit(X_train_cv, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:56:59.083339Z","iopub.execute_input":"2021-08-21T12:56:59.083786Z","iopub.status.idle":"2021-08-21T12:56:59.239226Z","shell.execute_reply.started":"2021-08-21T12:56:59.08375Z","shell.execute_reply":"2021-08-21T12:56:59.237964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:17:37.599546Z","iopub.execute_input":"2021-08-21T13:17:37.600141Z","iopub.status.idle":"2021-08-21T13:17:37.607502Z","shell.execute_reply.started":"2021-08-21T13:17:37.600103Z","shell.execute_reply":"2021-08-21T13:17:37.606273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:17:41.209833Z","iopub.execute_input":"2021-08-21T13:17:41.210379Z","iopub.status.idle":"2021-08-21T13:17:41.241703Z","shell.execute_reply.started":"2021-08-21T13:17:41.210343Z","shell.execute_reply":"2021-08-21T13:17:41.239858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix \nprint(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:20:46.025427Z","iopub.execute_input":"2021-08-21T13:20:46.025838Z","iopub.status.idle":"2021-08-21T13:20:46.039029Z","shell.execute_reply.started":"2021-08-21T13:20:46.025806Z","shell.execute_reply":"2021-08-21T13:20:46.038265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train_cv,y_train)\n\nY_pred_lr = lr.predict(X_test_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_lr = round(accuracy_score(Y_pred_lr,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\n\nsv = svm.SVC(kernel='linear')\n\nsv.fit(X_train_cv, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_svm = sv.predict(X_test_cv)\nscore_svm = round(accuracy_score(Y_pred_svm,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Linear SVM is: \"+str(score_svm)+\" %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_svm.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(20):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train_cv,y_train)\n    Y_pred_dt = dt.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\n\ndt = DecisionTreeClassifier(random_state=best_x)\ndt.fit(X_train_cv,y_train)\nY_pred_dt = dt.predict(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T05:03:36.239614Z","iopub.execute_input":"2021-09-12T05:03:36.240109Z","iopub.status.idle":"2021-09-12T05:03:36.353083Z","shell.execute_reply.started":"2021-09-12T05:03:36.24007Z","shell.execute_reply":"2021-09-12T05:03:36.351032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_dt = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmax_accuracy = 0\nfor x in range(20):\n    rf = RandomForestClassifier(random_state=x)\n    rf.fit(X_train_cv,y_train)\n    Y_pred_rf = rf.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\nrf = RandomForestClassifier(random_state=best_x)\nrf.fit(X_train_cv,y_train)\nY_pred_rf = rf.predict(X_test_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_rf = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_rf)+\" %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train_cv, y_train)\n\nY_pred_xgb = xgb_model.predict(X_test_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_xgb = round(accuracy_score(Y_pred_xgb,y_test)*100,2)\n\nprint(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = [score_lr,score_dt,score_rf,score_xgb]\nalgorithms = [\"Logistic Regression\",\"Naive Bayes\",\"Support Vector Machine\",\"K-Nearest Neighbors\",\"Decision Tree\",\"Random Forest\",\"XGBoost\"]    \n\nfor i in range(len(algorithms)):\n    print(\"The accuracy score achieved using \"+algorithms[i]+\" is: \"+str(scores[i])+\" %\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,7)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy score\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n    \n# for performing text clustering    \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# for providing the path\nimport os\nprint(os.listdir('../input/'))\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncolumns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question'],\n       ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:41:39.849575Z","iopub.execute_input":"2021-08-21T13:41:39.849976Z","iopub.status.idle":"2021-08-21T13:41:39.882898Z","shell.execute_reply.started":"2021-08-21T13:41:39.849943Z","shell.execute_reply":"2021-08-21T13:41:39.881779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data['length'] = training_data['sent'].apply(len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 7)\nsns.distplot(training_data['length'], color = 'purple')\nplt.title('The Distribution of Length over the Texts', fontsize = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wordcloud\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color = 'lightcyan',\n                      width = 1200,\n                      height = 700).generate(str(training_data['sent']))\n\nplt.figure(figsize = (15, 10))\nplt.imshow(wordcloud)\nplt.title(\"WordCloud \", fontsize = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ncv = CountVectorizer()\nwords = cv.fit_transform(training_data['sent'])\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\ncolor = plt.cm.twilight(np.linspace(0, 1, 20))\nfrequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = color)\nplt.title(\"Most Frequently Occuring Words - Top 20\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of X :\", words.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = cv.get_feature_names()\n\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\nY = cv.transform([\"you read this book\"])\nprediction = model.predict(Y)\nprint(\"Cluster number :\", prediction)\nY = cv.transform([\"what is favorite book\"])\nprediction = model.predict(Y)\nprint(\"Cluster number :\", prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\ndocuments = [\"This little kitty came to play when I was eating at a restaurant.\",\n             \"Merley has the best squooshy kitten belly.\",\n             \"Google Translate app is incredible.\",\n             \"If you open 100 tab in google you get a smiley face.\",\n             \"Best cat photo I've ever taken.\",\n             \"Climbing ninja cat.\",\n             \"Impressed with google map feedback.\",\n             \"Key promoter extension for Google Chrome.\"]\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\n\ntrue_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\n\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\n\nY = vectorizer.transform([\"chrome browser to open.\"])\nprediction = model.predict(Y)\nprint(prediction)\n\nY = vectorizer.transform([\"My cat is hungry.\"])\nprediction = model.predict(Y)\nprint(prediction)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:45:17.722208Z","iopub.execute_input":"2021-08-21T13:45:17.722599Z","iopub.status.idle":"2021-08-21T13:45:18.96992Z","shell.execute_reply.started":"2021-08-21T13:45:17.722566Z","shell.execute_reply":"2021-08-21T13:45:18.968929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\n\n\ndocuments = [\"This little kitty came to play when I was eating at a restaurant.\",\n             \"Merley has the best squooshy kitten belly.\",\n             \"Google Translate app is incredible.\",\n             \"If you open 100 tab in google you get a smiley face.\",\n             \"Best cat photo I've ever taken.\",\n             \"Climbing ninja cat.\",\n             \"Impressed with google map feedback.\",\n             \"Key promoter extension for Google Chrome.\"]\n\ntfidf = TfidfVectorizer(stop_words='english')\nX = tfidf.fit_transform(documents).todense()\n# transform the data matrix into pairwise distances list\ndist_array = pdist(X)\n# calculate hierarchy\n#Z = linkage(dist_array, 'ward')\n#plt.title(\"Ward\")\n#dendrogram(Z, labels=labels)\n\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Nomor Dokumen')\nplt.ylabel('Jarak Euclidean')\nplt.show()\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(X) \nprint(cluster.labels_)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T13:45:35.539764Z","iopub.execute_input":"2021-08-21T13:45:35.540101Z","iopub.status.idle":"2021-08-21T13:45:35.786684Z","shell.execute_reply.started":"2021-08-21T13:45:35.54007Z","shell.execute_reply":"2021-08-21T13:45:35.785725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import all libraries and dependencies for dataframe\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as plticker\n%matplotlib inline\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nfrom math import isnan\n\n# import all libraries and dependencies for clustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree\n\n# Reading the country file on which analysis needs to be done\n\ndf_country = pd.read_csv('../input/country-data/country-data.csv')\n\ndf_country.head()\n\n# Reading the data dictionary file\n\ndf_structure = pd.read_csv('../input/data_dictionary/data-dictionary.csv')\ndf_structure.head(10)\n\ndf_country.shape\n\ndf_country.describe()\n\ndf_country.info()\n\n# Calculating the Missing Values % contribution in DF\n\ndf_null = df_country.isna().mean()*100\ndf_null\n\n# Datatype check for the dataframe\n\ndf_country.dtypes\n\n# Duplicates check\n\ndf_country.loc[df_country.duplicated()]\n\n# Segregation of Numerical and Categorical Variables/Columns\n\ncat_col = df_country.select_dtypes(include = ['object']).columns\nnum_col = df_country.select_dtypes(exclude = ['object']).columns\n\n# Heatmap to understand the attributes dependency\n\nplt.figure(figsize = (15,10))        \nax = sns.heatmap(df_country.corr(),annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\n# Pairplot of all numeric columns\n\nsns.pairplot(df_country)\n\n# Converting exports,imports and health spending percentages to absolute values.\n\ndf_country['exports'] = df_country['exports'] * df_country['gdpp']/100\ndf_country['imports'] = df_country['imports'] * df_country['gdpp']/100\ndf_country['health'] = df_country['health'] * df_country['gdpp']/100\n\ndf_country.head(5)\n\n# Dropping Country field as final dataframe will only contain data columns\n\ndf_country_drop = df_country.copy()\ncountry = df_country_drop.pop('country')\n\ndf_country_drop.head()\n\n# Standarisation technique for scaling\n\nwarnings.filterwarnings(\"ignore\")\nscaler = StandardScaler()\ndf_country_scaled = scaler.fit_transform(df_country_drop)\n\ndf_country_scaled\n\npca = PCA(svd_solver='randomized', random_state=42)\n\n# Lets apply PCA on the scaled data\n\npca.fit(df_country_scaled)\n\n# PCA components created \n\npca.components_\n\n# Variance Ratio\n\npca.explained_variance_ratio_\n\n# Variance Ratio bar plot for each PCA components.\n\nax = plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\nplt.xlabel(\"PCA Components\",fontweight = 'bold')\nplt.ylabel(\"Variance Ratio\",fontweight = 'bold')\n\n# Scree plot to visualize the Cumulative variance against the Number of components\n\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.vlines(x=3, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=0.93, xmax=8, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.xlabel('Number of PCA components')\nplt.ylabel('Cumulative Explained Variance')\n\n# Checking which attributes are well explained by the pca components\n\norg_col = list(df_country.drop(['country'],axis=1).columns)\nattributes_pca = pd.DataFrame({'Attribute':org_col,'PC_1':pca.components_[0],'PC_2':pca.components_[1],'PC_3':pca.components_[2]})\n\nattributes_pca\n\n# Plotting the above dataframe for better visualization with PC1 and PC2\n\nsns.pairplot(data=attributes_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_2\"], hue = \"Attribute\" ,height=8)\nplt.xlabel(\"Principal Component 1\",fontweight = 'bold')\nplt.ylabel(\"Principal Component 2\",fontweight = 'bold')\n\nfor i,txt in enumerate(attributes_pca.Attribute):\n    plt.annotate(txt, (attributes_pca.PC_1[i],attributes_pca.PC_2[i]))\n\n# Plotting the above dataframe with PC1 and PC3 to understand the components which explains inflation.\n\nsns.pairplot(data=attributes_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_3\"], hue = \"Attribute\" ,height=8)\nplt.xlabel(\"Principal Component 1\",fontweight = 'bold')\nplt.ylabel(\"Principal Component 3\",fontweight = 'bold')\n\nfor i,txt in enumerate(attributes_pca.Attribute):\n    plt.annotate(txt, (attributes_pca.PC_1[i],attributes_pca.PC_3[i]))\n\n# Building the dataframe using Incremental PCA for better efficiency.\n\ninc_pca = IncrementalPCA(n_components=3)\n\n# Fitting the scaled df on incremental pca\n\ndf_inc_pca = inc_pca.fit_transform(df_country_scaled)\ndf_inc_pca\n\n# Creating new dataframe with Principal components\n\ndf_pca = pd.DataFrame(df_inc_pca, columns=[\"PC_1\", \"PC_2\",\"PC_3\"])\ndf_pca_final = pd.concat([country, df_pca], axis=1)\ndf_pca_final.head()\n\n# Plotting Heatmap to check is there still dependency in the dataset.\n\nplt.figure(figsize = (8,6))        \nax = sns.heatmap(df_pca_final.corr(),annot = True)\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\n\n# Scatter Plot to visualize the spread of data across PCA components\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1,3,1)\nsns.scatterplot(data=df_pca_final, x='PC_1', y='PC_2')\nplt.subplot(1,3,2)\nsns.scatterplot(data=df_pca_final, x='PC_1', y='PC_3')\nplt.subplot(1,3,3)\nsns.scatterplot(data=df_pca_final, x='PC_3', y='PC_2')\n\n# Outlier Analysis \n\noutliers = ['PC_1','PC_2','PC_3']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = df_pca_final[outliers], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"PC Components\", fontweight = 'bold')\n\n# Statstical Outlier treatment for PC_1\n\nQ1 = df_pca_final.PC_1.quantile(0.05)\nQ3 = df_pca_final.PC_1.quantile(0.95)\nIQR = Q3 - Q1\ndf_pca_final = df_pca_final[(df_pca_final.PC_1 >= Q1) & (df_pca_final.PC_1 <= Q3)]\n\n# Statstical Outlier treatment for PC_2\n\nQ1 = df_pca_final.PC_2.quantile(0.05)\nQ3 = df_pca_final.PC_2.quantile(0.95)\nIQR = Q3 - Q1\ndf_pca_final = df_pca_final[(df_pca_final.PC_2 >= Q1) & (df_pca_final.PC_2 <= Q3)]\n\n# Statstical Outlier treatment for PC_3\n\nQ1 = df_pca_final.PC_3.quantile(0.05)\nQ3 = df_pca_final.PC_3.quantile(0.95)\nIQR = Q3 - Q1\ndf_pca_final = df_pca_final[(df_pca_final.PC_3 >= Q1) & (df_pca_final.PC_3 <= Q3)]\n\n# Plot after Outlier removal \n\noutliers = ['PC_1','PC_2','PC_3']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = df_pca_final[outliers], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"PC Components\", fontweight = 'bold')\n\n# Reindexing the df after outlier removal\n\ndf_pca_final = df_pca_final.reset_index(drop=True)\ndf_pca_final_data = df_pca_final.drop(['country'],axis=1)\ndf_pca_final.head()\n\n# Calculating Hopkins score to know whether the data is good for clustering or not.\n\ndef hopkins(X):\n    d = X.shape[1]\n    n = len(X)\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    HS = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(HS):\n        print(ujd, wjd)\n        HS = 0\n \n    return HS\n\n# Elbow curve method to find the ideal number of clusters.\nssd = []\nfor num_clusters in list(range(1,8)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50,random_state= 100)\n    model_clus.fit(df_pca_final_data)\n    ssd.append(model_clus.inertia_)\n\nplt.plot(ssd)\n\n# Silhouette score analysis to find the ideal number of clusters for K-means clustering\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(df_pca_final_data)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_pca_final_data, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = []\nfor i in range(1, 4): \n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit(X_train_cv, y_train)\n pred_i = knn.predict(X_test_cv)\n #error.append(np.mean(pred_i != y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]}]}